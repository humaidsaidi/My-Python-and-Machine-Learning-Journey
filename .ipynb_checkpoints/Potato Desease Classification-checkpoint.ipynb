{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1053cd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 17:49:32.345976: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-19 17:49:35.703240: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-19 17:49:35.708895: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-19 17:49:45.828552: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from shutil import copyfile\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3362016a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/PLD_3_Classes_256/Training\n",
      "/tmp/PLD_3_Classes_256/Testing\n",
      "/tmp/PLD_3_Classes_256/Validation\n",
      "/tmp/PLD_3_Classes_256/Training/Early_Blight\n",
      "/tmp/PLD_3_Classes_256/Training/Late_Blight\n",
      "/tmp/PLD_3_Classes_256/Training/Healthy\n",
      "/tmp/PLD_3_Classes_256/Testing/Early_Blight\n",
      "/tmp/PLD_3_Classes_256/Testing/Late_Blight\n",
      "/tmp/PLD_3_Classes_256/Testing/Healthy\n",
      "/tmp/PLD_3_Classes_256/Validation/Early_Blight\n",
      "/tmp/PLD_3_Classes_256/Validation/Late_Blight\n",
      "/tmp/PLD_3_Classes_256/Validation/Healthy\n"
     ]
    }
   ],
   "source": [
    "root_dir = '/tmp/PLD_3_Classes_256'\n",
    "\n",
    "for rootdir, dirs, files in os.walk(root_dir):\n",
    "    for subdir in dirs:\n",
    "        print(os.path.join(rootdir, subdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7c6bb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*We Process the Training\n",
      "  we have a 1303 images from class Early_Blight\n",
      "  we have a 1132 images from class Late_Blight\n",
      "  we have a 816 images from class Healthy\n",
      "\n",
      "*We Process the Testing\n",
      "  we have a 162 images from class Early_Blight\n",
      "  we have a 141 images from class Late_Blight\n",
      "  we have a 102 images from class Healthy\n",
      "\n",
      "*We Process the Validation\n",
      "  we have a 163 images from class Early_Blight\n",
      "  we have a 151 images from class Late_Blight\n",
      "  we have a 102 images from class Healthy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let`s make EDA for the dataset\n",
    "data_splits = os.listdir(root_dir)\n",
    "for data_kind in data_splits:\n",
    "    classes = os.listdir(os.path.join(root_dir,data_kind))\n",
    "    print(f'*We Process the {data_kind}')\n",
    "    for class_ in classes:\n",
    "        path = os.path.join(os.path.join(root_dir,data_kind),class_)\n",
    "        print(f'  we have a {len(os.listdir(path))} images from class {class_}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8708f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*We Process the Training\n",
      "  Deleted 0 Images for Training\n",
      "\n",
      "*We Process the Testing\n",
      "  Deleted 0 Images for Testing\n",
      "\n",
      "*We Process the Validation\n",
      "  Deleted 0 Images for Validation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter out Corrupted images\n",
    "\n",
    "num_skipped = 0\n",
    "data_splits = os.listdir(root_dir)\n",
    "\n",
    "for data_kind in data_splits:\n",
    "    data_kind_path = os.path.join(root_dir,data_kind)\n",
    "    print(f'*We Process the {data_kind}')\n",
    "    \n",
    "    classes = os.listdir(data_kind_path)\n",
    "    for class_ in classes:\n",
    "        folder_path = os.path.join(data_kind_path,class_)\n",
    "        for fname in os.listdir(folder_path):\n",
    "            fpath = os.path.join(folder_path,fname)\n",
    "\n",
    "            try:\n",
    "                fobj = open(fpath,\"rb\")\n",
    "                is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n",
    "            finally:\n",
    "                fobj.close()\n",
    "\n",
    "            if not is_jfif:\n",
    "                num_skipped += 1\n",
    "                os.remove(fpath)\n",
    "    print(f\"  Deleted {num_skipped} Images for {data_kind}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9918bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DIR = os.path.join(root_dir, \"Training/\")\n",
    "VALIDATION_DIR = os.path.join(root_dir, \"Validation/\")\n",
    "TEST_DIR = os.path.join(root_dir, \"Testing/\")\n",
    "\n",
    "IMAGE_SIZE = (256, 256)\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "def train_val_test_generators(TRAINING_DIR, VALIDATION_DIR, TEST_DIR):\n",
    "  \"\"\"\n",
    "  Creates the training and validation data generators\n",
    "  \n",
    "  Args:\n",
    "    TRAINING_DIR (string): directory path containing the training images\n",
    "    VALIDATION_DIR (string): directory path containing the testing/validation images\n",
    "    \n",
    "  Returns:\n",
    "    train_generator, validation_generator - tuple containing the generators\n",
    "  \"\"\"\n",
    "  ### START CODE HERE\n",
    "  from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "  # Instantiate the ImageDataGenerator class (don't forget to set the rescale argument)\n",
    "  train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                     rotation_range=40,\n",
    "                                     width_shift_range=0.2,\n",
    "                                     height_shift_range=0.2,\n",
    "                                     shear_range=0.2,\n",
    "                                     zoom_range=0.2,\n",
    "                                     horizontal_flip=True,\n",
    "                                     fill_mode='nearest')\n",
    "\n",
    "  # Pass in the appropriate arguments to the flow_from_directory method\n",
    "  train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,\n",
    "                                                      batch_size=BATCH_SIZE,\n",
    "                                                      class_mode='categorical',\n",
    "                                                      target_size=IMAGE_SIZE)\n",
    "\n",
    "  # Instantiate the ImageDataGenerator class (don't forget to set the rescale argument)\n",
    "  validation_datagen = ImageDataGenerator(rescale=1.0/255.)\n",
    "\n",
    "  # Pass in the appropriate arguments to the flow_from_directory method\n",
    "  validation_generator = validation_datagen.flow_from_directory(directory=VALIDATION_DIR,\n",
    "                                                                batch_size=BATCH_SIZE,\n",
    "                                                                class_mode='categorical',\n",
    "                                                                target_size=(IMAGE_SIZE))\n",
    "\n",
    "  test_datagen = ImageDataGenerator(rescale=1.0/255.)\n",
    "  test_generator = test_datagen.flow_from_directory(directory=TEST_DIR,\n",
    "                                                                batch_size=BATCH_SIZE,\n",
    "                                                                class_mode='categorical',\n",
    "                                                                target_size=(IMAGE_SIZE))\n",
    "  ### END CODE HERE\n",
    "  return train_generator, validation_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6e6e63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3251 images belonging to 3 classes.\n",
      "Found 416 images belonging to 3 classes.\n",
      "Found 405 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Test your generators\n",
    "train_generator, validation_generator, test_generator = train_val_test_generators(TRAINING_DIR, VALIDATION_DIR, TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccf374b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DirectoryIterator' object has no attribute 'take'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# plot Visualize \u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, label \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtest_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m9\u001b[39m):\n\u001b[1;32m      5\u001b[0m         plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DirectoryIterator' object has no attribute 'take'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot Visualize \n",
    "test_list = list(test_generator)\n",
    "plt.figure(figsize=(10,10))\n",
    "for image, label in test_list.take(1):\n",
    "    for i in range(9):\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        plt.imshow(image[i].numpy().astype('uint8'))\n",
    "        plt.title(train_ds.class_names[label[i]])\n",
    "        plt.axis(\"off\");\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "464745d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# create a dataset from the generator\\ntrain_ds = tf.data.Dataset.from_generator(\\n    lambda: train_generator,\\n    output_types=(tf.float32, tf.float32),\\n    output_shapes=([None, 256, 256, 3], [20, 3])\\n)\\n\\n# apply the cache and prefetch methods to the dataset\\ntrain_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\\n\\n# repeat the process for the validation dataset\\nval_ds = tf.data.Dataset.from_generator(\\n    lambda: val_generator,\\n    output_types=(tf.float32, tf.float32),\\n    output_shapes=([None, 256, 256, 3], [20, 3])\\n)\\nval_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\\n\\n# repeat the process for the test dataset\\ntest_ds = tf.data.Dataset.from_generator(\\n    lambda: test_generator,\\n    output_types=(tf.float32, tf.float32),\\n    output_shapes=([None, 256, 256, 3], [20, 3])\\n)\\ntest_ds = test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# create a dataset from the generator\n",
    "train_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: train_generator,\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, 256, 256, 3], [20, 3])\n",
    ")\n",
    "\n",
    "# apply the cache and prefetch methods to the dataset\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# repeat the process for the validation dataset\n",
    "val_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: val_generator,\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, 256, 256, 3], [20, 3])\n",
    ")\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# repeat the process for the test dataset\n",
    "test_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: test_generator,\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, 256, 256, 3], [20, 3])\n",
    ")\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de47bcd5",
   "metadata": {},
   "source": [
    "The code above creates TensorFlow Dataset objects from the data generated by the train_generator and val_generator.\n",
    "\n",
    "The from_generator() method is used to create a dataset from a generator function. It takes in the generator function, lambda: train_generator, which returns the data in the form of a tuple of two tensors, the input tensor and the target tensor. The output_types argument specifies the data types of the output tensors, in this case tf.float32 for both input and target tensors. The output_shapes argument specifies the shapes of the tensors, which can vary in the first dimension (batch size) but are fixed for the other dimensions.\n",
    "\n",
    "After creating the dataset, the cache() method is applied to cache the data in memory to avoid I/O blocking, and the prefetch() method is applied to prefetch data to improve performance. The buffer_size argument of prefetch() specifies the number of elements to prefetch.\n",
    "\n",
    "The same process is repeated for the validation dataset by creating a val_ds dataset from the val_generator, and applying the same cache() and prefetch() methods.\n",
    "\n",
    "Overall, the code creates TensorFlow Dataset objects from the data generators and optimizes the performance of the dataset by caching and prefetching data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e5e30a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "INPUT_SHAPE = IMAGE_SIZE +(3,)\n",
    "\n",
    "def create_model():\n",
    "  # DEFINE A KERAS MODEL TO CLASSIFY CATS V DOGS\n",
    "  # USE AT LEAST 3 CONVOLUTION LAYERS\n",
    "\n",
    "  ### START CODE HERE\n",
    "\n",
    "  model = tf.keras.models.Sequential([ \n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape = INPUT_SHAPE),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    \n",
    "    # Flatten Block\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "  ])\n",
    "\n",
    "  \n",
    "  model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy']) \n",
    "    \n",
    "  ### END CODE HERE\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdb1475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Callback\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('val_accuracy')>=0.98):   \n",
    "            keys = list(logs.keys())\n",
    "            print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "# define object from CustomCallback Class\n",
    "callbacks = CustomCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f671494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a48b907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 16:27:46.428275: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 16:27:54.114181: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 165160960 exceeds 10% of free system memory.\n",
      "2023-04-19 16:27:56.065373: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 41290240 exceeds 10% of free system memory.\n",
      "2023-04-19 16:27:56.418395: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 80000000 exceeds 10% of free system memory.\n",
      "2023-04-19 16:27:57.332401: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 19681280 exceeds 10% of free system memory.\n",
      "2023-04-19 16:27:57.486428: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 18432000 exceeds 10% of free system memory.\n",
      "2023-04-19 16:28:04.242424: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: logits and labels must have the same first dimension, got logits shape [20,3] and labels shape [60]\n",
      "\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/humaid/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/humaid/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/home/humaid/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/home/humaid/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/home/humaid/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/humaid/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/humaid/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"/home/humaid/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/humaid/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/humaid/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_1414473/4116963302.py\", line 5, in <module>\n      history = model.fit(train_generator,\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1051, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n      return self.compiled_loss(\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/losses.py\", line 2078, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/backend.py\", line 5660, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [20,3] and labels shape [60]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_1717]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model()\n\u001b[1;32m      4\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/humaid/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/humaid/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/home/humaid/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/home/humaid/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/home/humaid/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/humaid/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/humaid/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"/home/humaid/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/humaid/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/humaid/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_1414473/4116963302.py\", line 5, in <module>\n      history = model.fit(train_generator,\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1051, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n      return self.compiled_loss(\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/losses.py\", line 2078, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"/home/humaid/.local/lib/python3.10/site-packages/keras/backend.py\", line 5660, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [20,3] and labels shape [60]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_1717]"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "model = create_model()\n",
    "\n",
    "EPOCHS = 50\n",
    "history = model.fit(train_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    epochs =EPOCHS,\n",
    "                    callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8b870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "print(\"\")\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(2,1,1)\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30af040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalute model\n",
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056a6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
